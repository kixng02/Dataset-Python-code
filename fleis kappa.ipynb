{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqosvlWvJNNR",
        "outputId": "0d43ee94-a698-454e-f02b-effef2d06eb8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import warnings\n",
        "\n",
        "class FleissKappaAnalyzer:\n",
        "    \"\"\"\n",
        "    A comprehensive Fleiss' Kappa calculator for assessing inter-rater agreement\n",
        "    among multiple AI models on indigenous plant classification.\n",
        "    \n",
        "    Formula: K = (P₀ - Pₑ) / (1 - Pₑ)\n",
        "    Where:\n",
        "      P₀ = observed agreement\n",
        "      Pₑ = expected agreement by chance\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.kappa = None\n",
        "        self.p_value = None\n",
        "        self.interpretation = None\n",
        "        \n",
        "    def validate_input(self, ratings):\n",
        "        \"\"\"Validate input data format and values\"\"\"\n",
        "        if not isinstance(ratings, (np.ndarray, pd.DataFrame, list)):\n",
        "            raise ValueError(\"Input must be numpy array, pandas DataFrame, or list\")\n",
        "            \n",
        "        ratings = np.array(ratings)\n",
        "        \n",
        "        if ratings.ndim != 2:\n",
        "            raise ValueError(\"Input must be 2-dimensional matrix\")\n",
        "            \n",
        "        if ratings.size == 0:\n",
        "            raise ValueError(\"Input matrix cannot be empty\")\n",
        "            \n",
        "        # Check for non-integer values\n",
        "        if not np.all(np.equal(np.mod(ratings, 1), 0)):\n",
        "            raise ValueError(\"All ratings must be integer values\")\n",
        "            \n",
        "        return ratings.astype(int)\n",
        "    \n",
        "    def calculate_fleiss_kappa(self, ratings, categories=None):\n",
        "        \"\"\"\n",
        "        Calculate Fleiss' Kappa for multiple raters.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        ratings : array-like\n",
        "            Matrix where rows are subjects and columns are raters\n",
        "        categories : list, optional\n",
        "            Category labels for interpretation\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Contains kappa, p_value, interpretation, and detailed results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Input validation and preprocessing\n",
        "            ratings = self.validate_input(ratings)\n",
        "            n, k = ratings.shape  # n subjects, k raters\n",
        "            \n",
        "            if n < 2 or k < 2:\n",
        "                raise ValueError(\"Need at least 2 subjects and 2 raters\")\n",
        "            \n",
        "            # Determine categories automatically if not provided\n",
        "            if categories is None:\n",
        "                categories = list(range(int(np.min(ratings)), int(np.max(ratings)) + 1))\n",
        "            m = len(categories)\n",
        "            \n",
        "            # Build frequency matrix\n",
        "            freq_matrix = np.zeros((n, m))\n",
        "            for i in range(n):\n",
        "                for j in range(m):\n",
        "                    freq_matrix[i, j] = np.sum(ratings[i] == categories[j])\n",
        "            \n",
        "            # Calculate observed agreement (P₀)\n",
        "            p0_numerator = 0\n",
        "            for i in range(n):\n",
        "                p0_numerator += np.sum(freq_matrix[i] * (freq_matrix[i] - 1))\n",
        "            P0 = p0_numerator / (n * k * (k - 1))\n",
        "            \n",
        "            # Calculate expected agreement (Pₑ)\n",
        "            p_j = np.sum(freq_matrix, axis=0) / (n * k)\n",
        "            Pe = np.sum(p_j ** 2)\n",
        "            \n",
        "            # Handle edge cases\n",
        "            if Pe == 1:\n",
        "                self.kappa = 1.0  # Perfect agreement\n",
        "            else:\n",
        "                self.kappa = (P0 - Pe) / (1 - Pe)\n",
        "            \n",
        "            # Calculate statistical significance\n",
        "            self.p_value = self._calculate_significance(n, k, P0, Pe)\n",
        "            \n",
        "            # Interpret results\n",
        "            self.interpretation = self._interpret_kappa(self.kappa)\n",
        "            \n",
        "            return {\n",
        "                'kappa': self.kappa,\n",
        "                'p_value': self.p_value,\n",
        "                'interpretation': self.interpretation,\n",
        "                'observed_agreement': P0,\n",
        "                'expected_agreement': Pe,\n",
        "                'n_subjects': n,\n",
        "                'n_raters': k,\n",
        "                'categories': categories\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating Fleiss' Kappa: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def _calculate_significance(self, n, k, P0, Pe):\n",
        "        \"\"\"Calculate approximate p-value for Fleiss' Kappa\"\"\"\n",
        "        if Pe == 1:\n",
        "            return 0.0\n",
        "            \n",
        "        # Standard error approximation\n",
        "        se = np.sqrt((2 * (1 - Pe)) / (n * k * (k - 1)))\n",
        "        z_score = self.kappa / se if se > 0 else 0\n",
        "        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))\n",
        "        \n",
        "        return p_value\n",
        "    \n",
        "    def _interpret_kappa(self, kappa):\n",
        "        \"\"\"Interpret Kappa value using Landis & Koch scale\"\"\"\n",
        "        if kappa < 0:\n",
        "            return \"Poor agreement (less than chance)\"\n",
        "        elif kappa <= 0.2:\n",
        "            return \"Slight agreement\"\n",
        "        elif kappa <= 0.4:\n",
        "            return \"Fair agreement\"\n",
        "        elif kappa <= 0.6:\n",
        "            return \"Moderate agreement\"\n",
        "        elif kappa <= 0.8:\n",
        "            return \"Substantial agreement\"\n",
        "        else:\n",
        "            return \"Almost perfect agreement\"\n",
        "    \n",
        "    def create_agreement_heatmap(self, ratings, plant_names, model_names):\n",
        "        \"\"\"Create a heatmap visualization of agreement patterns\"\"\"\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        \n",
        "        # Convert to agreement matrix (1 = all agree, 0 = disagree)\n",
        "        agreement_matrix = np.zeros(len(ratings))\n",
        "        \n",
        "        for i in range(len(ratings)):\n",
        "            # Check if all models agree (excluding no results)\n",
        "            valid_ratings = [r for r in ratings[i] if r != -1]\n",
        "            if len(valid_ratings) > 0:\n",
        "                agreement_matrix[i] = len(set(valid_ratings)) == 1\n",
        "            else:\n",
        "                agreement_matrix[i] = 0  # No valid ratings = disagreement\n",
        "        \n",
        "        # Reshape for heatmap\n",
        "        agreement_display = np.tile(agreement_matrix, (3, 1)).T\n",
        "        \n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(agreement_display, \n",
        "                   xticklabels=model_names,\n",
        "                   yticklabels=plant_names,\n",
        "                   cmap=['red', 'green'],\n",
        "                   cbar_kws={'label': 'Agreement (Red=Disagree, Green=Agree)'})\n",
        "        \n",
        "        plt.title('Inter-Model Agreement Patterns on Indigenous Plant Classification')\n",
        "        plt.xlabel('AI Models')\n",
        "        plt.ylabel('Indigenous Plants')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def create_detailed_heatmap(self, ratings, plant_names, model_names):\n",
        "        \"\"\"Create a detailed heatmap showing actual classifications\"\"\"\n",
        "        # Convert numerical ratings to descriptive labels for visualization\n",
        "        label_map = {0: 'Medicinal', 1: 'Edible', 2: 'Poisonous', -1: 'No Results'}\n",
        "        \n",
        "        rating_labels = []\n",
        "        for plant_ratings in ratings:\n",
        "            labels = [label_map[r] for r in plant_ratings]\n",
        "            rating_labels.append(labels)\n",
        "        \n",
        "        rating_df = pd.DataFrame(rating_labels, \n",
        "                               index=plant_names, \n",
        "                               columns=model_names)\n",
        "        \n",
        "        plt.figure(figsize=(14, 10))\n",
        "        \n",
        "        # Create custom colormap\n",
        "        from matplotlib.colors import ListedColormap\n",
        "        cmap = ListedColormap(['#2E8B57', '#FFD700', '#DC143C', '#696969'])\n",
        "        \n",
        "        sns.heatmap(rating_df.apply(lambda x: pd.Categorical(x).codes), \n",
        "                   cmap=cmap,\n",
        "                   xticklabels=model_names,\n",
        "                   yticklabels=plant_names,\n",
        "                   cbar_kws={'ticks': [0, 1, 2, 3], \n",
        "                           'label': 'Classification'})\n",
        "        \n",
        "        # Customize colorbar labels\n",
        "        cbar = plt.gca().collections[0].colorbar\n",
        "        cbar.set_ticklabels(['Medicinal', 'Edible', 'Poisonous', 'No Results'])\n",
        "        \n",
        "        plt.title('Detailed AI Model Classifications of Indigenous Plants\\n(Actual Data from Table 1)')\n",
        "        plt.xlabel('AI Models')\n",
        "        plt.ylabel('Indigenous Plants')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return rating_df\n",
        "    \n",
        "    def generate_report(self, results, plant_names):\n",
        "        \"\"\"Generate a comprehensive analysis report\"\"\"\n",
        "        if results is None:\n",
        "            return \"Analysis failed - invalid input data\"\n",
        "        \n",
        "        report = f\"\"\"\n",
        "        FLEISS' KAPPA ANALYSIS REPORT - INDIGENOUS PLANT CLASSIFICATION\n",
        "        {'=' * 60}\n",
        "        \n",
        "        Dataset Summary:\n",
        "        - Number of indigenous plants: {results['n_subjects']}\n",
        "        - Number of AI models: {results['n_raters']}\n",
        "        - Categories: {['No Results', 'Medicinal', 'Edible', 'Poisonous']}\n",
        "        \n",
        "        Agreement Statistics:\n",
        "        - Fleiss' Kappa: {results['kappa']:.3f}\n",
        "        - Observed Agreement (P₀): {results['observed_agreement']:.3f}\n",
        "        - Expected Agreement (Pₑ): {results['expected_agreement']:.3f}\n",
        "        - Statistical Significance: p = {results['p_value']:.4f}\n",
        "        \n",
        "        Interpretation:\n",
        "        - {results['interpretation']}\n",
        "        - This indicates {('significant' if results['p_value'] < 0.05 else 'non-significant')} inter-model reliability\n",
        "        - The models show {results['interpretation'].lower()} in classifying indigenous plants\n",
        "        \n",
        "        RESEARCH IMPLICATIONS:\n",
        "        • This kappa value demonstrates the level of consistency across AI models\n",
        "        • Low agreement highlights bias and lack of standardized IKS knowledge\n",
        "        • Supports the need for integrating Indigenous Knowledge Systems into AI training\n",
        "        \"\"\"\n",
        "        \n",
        "        return report\n",
        "\n",
        "# ACTUAL DATA FROM TABLE 1 - Converted to numerical format\n",
        "def prepare_actual_classification_data():\n",
        "    \"\"\"\n",
        "    Convert actual Table 1 data into numerical format for Fleiss' Kappa\n",
        "    Encoding: 0 = Medicinal, 1 = Edible, 2 = Poisonous, -1 = No results/Not accurate\n",
        "    \"\"\"\n",
        "    \n",
        "    # Plant names from Table 1\n",
        "    plant_names = [\n",
        "        \"Aloe ferox\", \"African ginger\", \"Wild rosemary\", \"Devil's claw\", \n",
        "        \"African wormwood\", \"Pepperbark tree\", \"Pineapple flower\", \"Spekboom\",\n",
        "        \"False horsewood\", \"Sand raisin\", \"Mountain nettle\", \"Acacia\",\n",
        "        \"River karee\", \"Kudu lily\", \"Waterberg raisin\", \"Sweet wild garlic\",\n",
        "        \"Cyrtanthus sanguineus\", \"Ruttya fruticosa\", \"Sesamum trilobum\", \"Aloe hahnii\"\n",
        "    ]\n",
        "    \n",
        "    # Actual classifications from Table 1 converted to numerical codes\n",
        "    # Columns: [ChatGPT, Gemini, Mistral AI]\n",
        "    # Encoding: 0=Medicinal, 1=Edible, 2=Poisonous, -1=No results/Not accurate\n",
        "    actual_classifications = [\n",
        "        [0, 0, 1],       # Aloe ferox: Medicinal, Medicinal, Edible\n",
        "        [0, 0, 0],       # African ginger: All Medicinal\n",
        "        [-1, -1, -1],    # Wild rosemary: All No results\n",
        "        [0, 0, 0],       # Devil's claw: All Medicinal\n",
        "        [0, 0, 0],       # African wormwood: All Medicinal\n",
        "        [-1, -1, -1],    # Pepperbark tree: All Not accurate (treated as disagreement)\n",
        "        [0, 0, 0],       # Pineapple flower: All Medicinal\n",
        "        [-1, -1, -1],    # Spekboom: All Not accurate\n",
        "        [2, -1, -1],     # False horsewood: Poisonous, No results, Not accurate\n",
        "        [-1, -1, 1],     # Sand raisin: No results, Not accurate, Edible\n",
        "        [-1, -1, -1],    # Mountain nettle: All Not accurate\n",
        "        [0, 2, 2],       # Acacia: Medicinal, Poisonous, Poisonous\n",
        "        [0, -1, -1],     # River karee: Medicinal, No results, Not accurate\n",
        "        [0, -1, 0],      # Kudu lily: Medicinal, Not accurate, Medicinal\n",
        "        [-1, -1, -1],    # Waterberg raisin: All Not accurate\n",
        "        [-1, -1, -1],    # Sweet wild garlic: All No results\n",
        "        [-1, -1, 1],     # Cyrtanthus sanguineus: Not accurate, Not accurate, Edible\n",
        "        [-1, 0, -1],     # Ruttya fruticosa: No results, Medicinal, Not accurate\n",
        "        [-1, -1, 1],     # Sesamum trilobum: No results, No results, Edible\n",
        "        [-1, 0, -1]      # Aloe hahnii: No results, Medicinal, Not accurate\n",
        "    ]\n",
        "    \n",
        "    return actual_classifications, plant_names\n",
        "\n",
        "# MAIN EXECUTION\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize analyzer\n",
        "    analyzer = FleissKappaAnalyzer()\n",
        "    \n",
        "    # Prepare ACTUAL data from Table 1\n",
        "    ratings, plant_names = prepare_actual_classification_data()\n",
        "    model_names = [\"ChatGPT\", \"Gemini\", \"Mistral AI\"]\n",
        "    \n",
        "    print(\"ANALYZING ACTUAL AI CLASSIFICATION DATA FROM TABLE 1\")\n",
        "    print(\"=\" * 55)\n",
        "    \n",
        "    # Calculate Fleiss' Kappa with actual data\n",
        "    results = analyzer.calculate_fleiss_kappa(ratings, categories=[-1, 0, 1, 2])\n",
        "    \n",
        "    # Generate and display comprehensive report\n",
        "    if results:\n",
        "        report = analyzer.generate_report(results, plant_names)\n",
        "        print(report)\n",
        "        \n",
        "        # Create visualizations\n",
        "        print(\"\\nGenerating Agreement Visualization...\")\n",
        "        analyzer.create_agreement_heatmap(ratings, plant_names, model_names)\n",
        "        \n",
        "        print(\"\\nGenerating Detailed Classification Heatmap...\")\n",
        "        detailed_df = analyzer.create_detailed_heatmap(ratings, plant_names, model_names)\n",
        "        \n",
        "        # Save results for dissertation\n",
        "        results_df = pd.DataFrame({\n",
        "            'Metric': ['Fleiss Kappa', 'P-value', 'Observed Agreement', 'Expected Agreement', 'Interpretation'],\n",
        "            'Value': [f\"{results['kappa']:.3f}\", \n",
        "                     f\"{results['p_value']:.4f}\", \n",
        "                     f\"{results['observed_agreement']:.3f}\", \n",
        "                     f\"{results['expected_agreement']:.3f}\",\n",
        "                     results['interpretation']]\n",
        "        })\n",
        "        \n",
        "        print(\"\\nRESULTS SUMMARY FOR DISSERTATION:\")\n",
        "        print(results_df)\n",
        "        \n",
        "        # Additional analysis: Agreement rate\n",
        "        total_agreements = 0\n",
        "        for plant_ratings in ratings:\n",
        "            valid_ratings = [r for r in plant_ratings if r != -1]\n",
        "            if len(valid_ratings) > 0 and len(set(valid_ratings)) == 1:\n",
        "                total_agreements += 1\n",
        "        \n",
        "        agreement_rate = total_agreements / len(ratings)\n",
        "        print(f\"\\nAdditional Metrics:\")\n",
        "        print(f\"• Overall Agreement Rate: {agreement_rate:.1%} ({total_agreements}/{len(ratings)} plants)\")\n",
        "        print(f\"• Number of Plants with Full Consensus: {total_agreements}\")\n",
        "        print(f\"• Number of Plants with Disagreement: {len(ratings) - total_agreements}\")\n",
        "        \n",
        "        # Save detailed results to CSV for inclusion in appendix\n",
        "        detailed_results = []\n",
        "        for i, plant in enumerate(plant_names):\n",
        "            detailed_results.append({\n",
        "                'Plant_Name': plant,\n",
        "                'ChatGPT': ['No Results', 'Medicinal', 'Edible', 'Poisonous'][ratings[i][0] + 1],\n",
        "                'Gemini': ['No Results', 'Medicinal', 'Edible', 'Poisonous'][ratings[i][1] + 1],\n",
        "                'Mistral_AI': ['No Results', 'Medicinal', 'Edible', 'Poisonous'][ratings[i][2] + 1],\n",
        "                'Consensus': 'Yes' if len(set([r for r in ratings[i] if r != -1])) == 1 else 'No'\n",
        "            })\n",
        "        \n",
        "        detailed_df = pd.DataFrame(detailed_results)\n",
        "        detailed_df.to_csv('ai_model_classification_analysis.csv', index=False)\n",
        "        print(f\"\\nDetailed results saved to 'ai_model_classification_analysis.csv'\")\n",
        "   "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
